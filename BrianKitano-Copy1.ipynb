{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brian Kitano Coding Test\n",
    "\n",
    "Start: 9:23 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Start: 9:23 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central task is to collapse the grid-level dataset into a district-level dataset.\n",
    "\n",
    "The final product from this section is a district-level daily dataset from 2009-2013 with temperature, rainfall, and total rainfall variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Calculate the set of points $P$ to include for each district.\n",
    "2. Use the formulas to calculate the district statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual EPIC project used 339 districts in India, 112 years of data and a 0.25°(latitude) x 0.25°(longitude) grid for rainfall. How would your code scale up with this larger dataset? Would you need any additional computing resources? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to go with the minimal possible answer to minize the scope of queries, which will scale better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: weighted average of daily mean temp, mean rainfall, and total rainfall for all grid points w/in 100km of each district's geographic center. Weights are inverse of the squared distance from the district center.\n",
    "\n",
    "$$\n",
    "\\bar t = \\frac{1}{|P|} \\sum_{p \\in P} \\frac{t_p}{(d - p)^2}\n",
    "$$\n",
    "$$\n",
    "\\bar r = \\frac{1}{|P|} \\sum_{p \\in P} \\frac{r_p}{(d - p)^2}\n",
    "$$\n",
    "$$\n",
    "R = \\sum_{p \\in P} \\frac{r_p}{(d - p)^2}\n",
    "$$\n",
    "\n",
    "where $P$ is the set of points within 100km of the district centroid, $d$ is the district centroid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "- Are the `rainfall` csv's identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rain_df = pd.read_csv('./data/Rainfall/rainfall_2010.csv')\n",
    "raw_temp_df = pd.read_csv('./data/Temperature/temperature_2010.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do our analysis, we need to ensure that every entry has at least a day, month, year, latitude and longitude. Let's remove any that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDefinedRows(df):\n",
    "    rows_with_data = \\\n",
    "        pd.notna(df['latitude']) & \\\n",
    "        pd.notna(df['longitude']) & \\\n",
    "        pd.notna(df['day']) & \\\n",
    "        pd.notna(df['month']) & \\\n",
    "        pd.notna(df['year'])\n",
    "    \n",
    "    return rows_with_data\n",
    "\n",
    "rain_df = raw_rain_df[getDefinedRows(raw_rain_df)]\n",
    "temp_df = raw_temp_df[getDefinedRows(raw_temp_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also assuming that in any 'year' file, there is actually only one calendar year represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to create an `index` column in both dataframes so that we can join them. The index needs to be identical in both data frames. Note that the `date` column is formatted differently in each dataframe, but `day`, `month`, and `year` are the same. Our index will be `{lat}_{long}_{month}_{day}_{year}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_columns = ['latitude', 'longitude', 'day', 'month', 'year']\n",
    "total_df = rain_df.merge(temp_df, how='outer', left_on=on_columns, right_on=on_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(total_df) == len(rain_df) & len(total_df) == len(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rainfall values: 112785\n",
      "temp values: 128471\n"
     ]
    }
   ],
   "source": [
    "rainfall_values = sum(pd.notna(total_df['rainfall']))\n",
    "print(f'rainfall values: {rainfall_values}')\n",
    "\n",
    "temp_values = sum(pd.notna(total_df['temperature']))\n",
    "print(f'temp values: {temp_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
