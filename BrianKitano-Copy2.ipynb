{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy import distance\n",
    "import numpy as np\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brian Kitano Coding Test\n",
    "\n",
    "Start: 9:23 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Start: 9:23 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central task is to collapse the grid-level dataset into a district-level dataset.\n",
    "\n",
    "The final product from this section is a district-level daily dataset from 2009-2013 with temperature, rainfall, and total rainfall variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Calculate the set of points $P$ to include for each district.\n",
    "2. Use the formulas to calculate the district statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual EPIC project used 339 districts in India, 112 years of data and a 0.25°(latitude) x 0.25°(longitude) grid for rainfall. How would your code scale up with this larger dataset? Would you need any additional computing resources? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "- Are the `rainfall` csv's identical?\n",
    "- I'm also assuming that in any 'year' file, there is actually only one calendar year represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to go with the minimal possible answer to minize the scope of queries, which will scale better. Also, we don't want our functions to rely on having to store a merged data frame in memory, since if there were 339 districts, and 112 years of data, the data frames would be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: weighted average of daily mean temp, mean rainfall, and total rainfall for all grid points w/in 100km of each district's geographic center. Weights are inverse of the squared distance from the district center.\n",
    "\n",
    "$$\n",
    "\\bar t = \\frac{1}{|P|} \\sum_{p \\in P} \\frac{t_p}{(d - p)^2}\n",
    "$$\n",
    "$$\n",
    "\\bar r = \\frac{1}{|P|} \\sum_{p \\in P} \\frac{r_p}{(d - p)^2}\n",
    "$$\n",
    "$$\n",
    "R = \\sum_{p \\in P} \\frac{r_p}{(d - p)^2}\n",
    "$$\n",
    "\n",
    "where $P$ is the set of points within 100km of the district centroid, $d$ is the district centroid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rain_df = pd.read_csv('./data/Rainfall/rainfall_2010.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do our analysis, we need to ensure that every entry has at least a day, month, year, latitude and longitude. Let's remove any that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDefinedRows(df):\n",
    "    rows_with_data = \\\n",
    "        pd.notna(df['latitude']) & \\\n",
    "        pd.notna(df['longitude']) & \\\n",
    "        pd.notna(df['day']) & \\\n",
    "        pd.notna(df['month']) & \\\n",
    "        pd.notna(df['year'])\n",
    "    \n",
    "    return rows_with_data\n",
    "\n",
    "rain_df = raw_rain_df[getDefinedRows(raw_rain_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the haversine function takes so long to compute, we don't want to compute it twice for both rainfall and temperature, so we should merge the dataframes. In fact, we can store the haversine distances for all the points and centroids so it only needs to be computed once. There are 961 points and 339 districts, which amounts to ~325k distance calculations. If I had the time, I would try to replicate the problem outlined here: https://iliauk.wordpress.com/2016/02/16/millions-of-distances-high-performance-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = pd.read_csv('./data/Geo/district_crosswalk_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df = rain_df[['latitude', 'longitude']].drop_duplicates(subset=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df['uid'] = [hex(i) for i in range(len(points_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stname_iaa</th>\n",
       "      <th>distname_iaa</th>\n",
       "      <th>stid_iaa</th>\n",
       "      <th>distid_iaa</th>\n",
       "      <th>centroid_longitude</th>\n",
       "      <th>centroid_latitude</th>\n",
       "      <th>unique_dist_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gujarat</td>\n",
       "      <td>ahmedabad</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>72.267197</td>\n",
       "      <td>22.781200</td>\n",
       "      <td>1711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gujarat</td>\n",
       "      <td>amreli</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>71.177803</td>\n",
       "      <td>21.373199</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gujarat</td>\n",
       "      <td>banaskantha</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>71.939903</td>\n",
       "      <td>24.219601</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gujarat</td>\n",
       "      <td>baroda</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>73.532700</td>\n",
       "      <td>22.230101</td>\n",
       "      <td>1714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gujarat</td>\n",
       "      <td>bhavnagar</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>71.770798</td>\n",
       "      <td>21.594500</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stname_iaa distname_iaa  stid_iaa  distid_iaa  centroid_longitude  \\\n",
       "0    gujarat    ahmedabad        17          11           72.267197   \n",
       "1    gujarat       amreli        17           5           71.177803   \n",
       "2    gujarat  banaskantha        17           8           71.939903   \n",
       "3    gujarat       baroda        17          14           73.532700   \n",
       "4    gujarat    bhavnagar        17           4           71.770798   \n",
       "\n",
       "   centroid_latitude  unique_dist_id  \n",
       "0          22.781200            1711  \n",
       "1          21.373199            1705  \n",
       "2          24.219601            1708  \n",
       "3          22.230101            1714  \n",
       "4          21.594500            1704  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df[['lat_radians', 'lon_radians']] = (np.radians(geo_df.loc[:, ['centroid_latitude','centroid_longitude']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df[['lat_radians', 'lon_radians']] = (np.radians(points_df.loc[:, ['latitude','longitude']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = sklearn.neighbors.DistanceMetric.get_metric('haversine')\n",
    "\n",
    "dist_matrix = (dist.pairwise\n",
    "    (geo_df[['lat_radians','lon_radians']],\n",
    "     points_df[['lat_radians','lon_radians']])*6367\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_matrix = pd.DataFrame(dist_matrix, \n",
    "                 index=geo_df['unique_dist_id'], \n",
    "                 columns=points_df['uid']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_long = pd.melt(df_dist_matrix.reset_index(),id_vars='unique_dist_id')\n",
    "df_dist_long = df_dist_long.rename(columns={'value':'km'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = df_dist_long.merge(points_df, how='outer', on='uid').drop(columns=['lat_radians', 'lon_radians'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just load our distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.read_csv('./distances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
